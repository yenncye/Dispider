# <img src="img/logo.png" style="vertical-align: -10px;" :height="40px" width="40px"> Dispider
This repository is the official implementation of Dispider.


<img align="center" src="img/pipeline.png" style="  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 100%;" />

<p align="center" style="font-size: em; margin-top: 0.5em">

[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC_BY--NC_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)<br>
<a href="https://arxiv.org/abs/xxxx"><img src="https://img.shields.io/badge/arXiv-paper-<color>"></a>
<a href="assets/paper.pdf"><img src="https://img.shields.io/badge/PDF-red"></a>
<!-- <a href="https://mark12ding.github.io/project/SAM2Long/"><img src="https://img.shields.io/badge/Project-Homepage-green"></a> -->
<!-- <a href="https://huggingface.co/spaces/Mar2Ding/SAM2Long-Demo"><img src="https://img.shields.io/badge/ü§óHugging Demo-yellow"></a> -->
</p>



>[**Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction**](https://arxiv.org/abs/xxxxx)<br>
> [Rui Qian](https://shvdiwnkozbw.github.io/), [Shuangrui Ding](https://mark12ding.github.io/), [Xiaoyi Dong](https://lightdxy.github.io/), [Pan Zhang](https://panzhang0212.github.io/)<br>
[Yuhang Zang](https://yuhangzang.github.io/), [Yuhang Cao](https://scholar.google.com/citations?user=sJkqsqkAAAAJ), [Dahua Lin](http://dahua.site/), [Jiaqi Wang](https://myownskyw7.github.io/)<br>
CUHK, Shanghai AI Lab


## üì∞ News
- [2025/1/6] üî•üî•üî• We released the paper on arXiv!

## üßæ ToDo Lists
- [x] Release Inference Code
- [ ] Release Checkpoints
- [ ] Release Training Code
- [ ] Release Demo Video


## üí° Highlights
### üî• A New Paradigm for Online Video LLMs with Active Real-Time Interaction
Dispider enables real-time interactions with streaming videos, unlike traditional offline video LLMs that process the entire video before responding. It provides continuous, timely feedback in live scenarios.

### ‚ö°Ô∏è Disentangled Perception, Decision, and Reaction Modules Operating Asynchronously
Dispider separates perception, decision-making, and reaction into asynchronous modules that operate in parallel. This ensures continuous video processing and response generation without blocking, enabling timely interactions.


### ü§Ø Superior Performance on StreamingBench and Conventional Video Benchmarks
Dispider outperforms VideoLLM-online on StreamingBench and surpasses offline Video LLMs on benchmarks like EgoSchema, VideoMME, MLVU, and ETBench. It excels in temporal reasoning and handles diverse video lengths effectively.

## üõ†Ô∏è Installation
Follow the steps below to set up the Dispider environment. We recommend using the specified versions of each library to ensure reproduce optimal performance.

### 1. **Create and Activate a Conda Environment**

First, create a new Conda environment with Python 3.10 and activate it:

```bash
conda create -n dispider python=3.10 -y
conda activate dispider
```

### 2. Upgrade pip

Ensure that `pip` is up to date to avoid any installation issues:

```bash
pip install --upgrade pip
```


### 3. Install Required Libraries
Ensure that CUDA 11.8 is installed on your system. You can download it from the [official NVIDIA website](https://developer.nvidia.com/cuda-11-8-0-download-archive). Follow the installation instructions provided there.

```bash
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0

pip install flash-attn==2.5.9.post1 transformers==4.41.2 deepspeed==0.9.5 accelerate==0.27.2 pydantic==1.10.13 timm==0.6.13
```

## Quick Start
First download the checkpoints at the folder. 

To perform single-turn inference, execute the following script:
```bash
python inference.py --model_path YOUR_MODEL_PATH --video_path YOUR_VIDEO_PATH --prompt YOUR_PROMPT
```
By default, the prompt is inserted at the beginning of the streaming video. The expected response will be generated in a single turn.

## Example Evaluation of VideoMME
Update the `video_path` in `data/videomme_template.json` and adjust the corresponding argument in `videomme.sh`. Then execute the following command, which will utilize 8 GPUs to run the inference in parallel:
```bash
bash scripts/eval/videomme.sh
```


## ‚òéÔ∏è Contact
Shuangrui Ding: mark12ding@gmail.com


## üîí License
The majority of this project is released under the CC-BY-NC 4.0 license as found in the LICENSE file. 


## üëç Acknowledgements
This codebase is built upon [LLaVA](https://github.com/haotian-liu/LLaVA) and leverages several open-source libraries. We extend our gratitude to the contributors and maintainers of these projects.

<!-- 
## ‚úíÔ∏è Citation
If you find our work helpful for your research, please consider giving a star ‚≠ê and citation üìù.
```bibtex

``` -->


